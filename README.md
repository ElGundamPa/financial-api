# Financial Data API v2.1.0 - Serverless Ready

Una API optimizada para obtener datos financieros de m√∫ltiples fuentes (Finviz, Yahoo Finance) con scraping HTTP-only, cache en memoria ef√≠mero y optimizada para despliegue serverless en Vercel.

## üöÄ Caracter√≠sticas

- **M√∫ltiples fuentes de datos**: Finviz, Yahoo Finance (HTTP-only)
- **Scraping as√≠ncrono**: Ejecuci√≥n paralela para mejor rendimiento
- **Cache en memoria**: TTL ef√≠mero optimizado para serverless
- **Sin base de datos**: Todo viaja en request/response
- **Rate limiting**: Protecci√≥n contra spam
- **CORS configurado**: Seguridad para frontends
- **Compresi√≥n GZip**: Optimizaci√≥n de transferencia
- **Timeouts estrictos**: Robustez en entornos serverless
- **API RESTful**: Endpoints organizados y documentados
- **Vercel Ready**: Despliegue serverless optimizado

## üìã Requisitos

- Python 3.12+
- Conexi√≥n a internet
- Vercel CLI (para despliegue)

## üõ†Ô∏è Instalaci√≥n

1. **Clonar el repositorio**
```bash
git clone <tu-repositorio>
cd financial_api
```

2. **Crear entorno virtual**
```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate
```

3. **Instalar dependencias**
```bash
pip install -r requirements.txt
```

4. **Configurar variables de entorno (opcional)**
```bash
# Copiar archivo de ejemplo
cp env.example .env
# Editar seg√∫n necesidades
```

## üöÄ Uso

### Opci√≥n 1: Docker (Recomendado)

```bash
# Usar docker-compose (incluye Redis)
docker-compose up -d

# O solo la API
docker build -t financial-api .
docker run -p 8000:8000 financial-api
```

### Opci√≥n 2: Local con Redis

```bash
# Instalar Redis
# Ubuntu/Debian
sudo apt-get install redis-server

# macOS
brew install redis

# Iniciar la API
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

### Opci√≥n 3: Local sin Redis (fallback a memoria)

```bash
# Iniciar la API
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

**Endpoints disponibles:**
- `GET /` - Informaci√≥n de la API
- `GET /datos` - Obtener todos los datos financieros
- `GET /datos/resume` - Obtener resumen de datos
- `GET /sources` - Informaci√≥n de fuentes disponibles
- `POST /scrape` - Ejecutar scraping manualmente
- `GET /health` - Verificar estado de la API

## üöÄ Despliegue en Vercel

### Opci√≥n 1: Despliegue Autom√°tico

1. **Conectar repositorio en Vercel**
   - Ve a [vercel.com](https://vercel.com)
   - Conecta tu repositorio de GitHub
   - Selecciona la rama `deploy/vercel`

2. **Configurar variables de entorno**
   ```bash
   CORS_ORIGINS=https://tu-frontend.com
   RATE_LIMIT_RPM=60
   HTTP_TIMEOUT_SECONDS=12
   CACHE_TTL_SECONDS=90
   MAX_BODY_KB=128
   ```

3. **Desplegar**
   - Vercel detectar√° autom√°ticamente la configuraci√≥n
   - La API estar√° disponible en `https://tu-proyecto.vercel.app`

### Opci√≥n 2: Despliegue Manual

1. **Instalar Vercel CLI**
   ```bash
   npm i -g vercel
   ```

2. **Login en Vercel**
   ```bash
   vercel login
   ```

3. **Desplegar**
   ```bash
   vercel --prod
   ```

### Desarrollo Local

```bash
# Usar Makefile
make dev

# O directamente
uvicorn api.index:app --reload --host 0.0.0.0 --port 8000
```

### Variables de Entorno

| Variable | Descripci√≥n | Default |
|----------|-------------|---------|
| `CORS_ORIGINS` | Or√≠genes permitidos para CORS | `http://localhost:3000` |
| `RATE_LIMIT_RPM` | Requests por minuto | `60` |
| `HTTP_TIMEOUT_SECONDS` | Timeout para requests HTTP | `12` |
| `CACHE_TTL_SECONDS` | TTL del cache en memoria | `90` |
| `MAX_BODY_KB` | Tama√±o m√°ximo de body en KB | `128` |

### Limitaciones Serverless

- **Cache ef√≠mero**: Los datos se pierden entre invocaciones
- **Sin TradingView**: Requiere navegador, deshabilitado en Vercel
- **Timeouts**: M√°ximo 10 segundos por funci√≥n
- **Memoria**: L√≠mite de 1024MB por funci√≥n

### Opci√≥n 2: API Separada + Bot

Si prefieres separar el scraping de la API:

```bash
# Terminal 1: API para recibir datos
uvicorn api:app --host 0.0.0.0 --port 8000 --reload

# Terminal 2: Bot de scraping (ejecutar cuando necesites datos)
python bot_scraper.py
```

## üìä Estructura de Datos

Los datos se almacenan en la siguiente estructura:

```json
{
  "tradingview": {
    "indices": [...],
    "acciones": [...],
    "cripto": [...],
    "forex": [...]
  },
  "finviz": {
    "forex": [...],
    "acciones": [...],
    "indices": [...]
  },
  "yahoo": {
    "forex": [...],
    "acciones": [...],
    "materias_primas": [...],
    "indices": [...]
  },
  "last_updated": "2024-01-01T12:00:00",
  "metadata": {
    "version": "1.0",
    "created_at": "2024-01-01T00:00:00"
  }
}
```

## ‚öôÔ∏è Configuraci√≥n

### Variables de Entorno

| Variable | Descripci√≥n | Valor por defecto |
|----------|-------------|-------------------|
| `API_HOST` | Host de la API | localhost |
| `API_PORT` | Puerto de la API | 8000 |
| `SCRAPING_INTERVAL_MINUTES` | Intervalo de scraping | 50 |
| `REQUEST_TIMEOUT` | Timeout para requests | 15 |
| `BROWSER_TIMEOUT` | Timeout para navegador | 60000 |
| `DATA_FILE` | Archivo de datos | data.json |
| `BACKUP_FILE` | Archivo de backup | data_backup.json |
| `LOG_LEVEL` | Nivel de logging | INFO |
| `LOG_FILE` | Archivo de logs | financial_api.log |
| `ENABLE_SCREENSHOTS` | Habilitar screenshots | true |
| `CORS_ORIGINS` | Or√≠genes permitidos para CORS | http://localhost:3000,http://127.0.0.1:3000 |
| `CORS_ALLOW_CREDENTIALS` | Permitir credenciales en CORS | false |
| `CORS_ALLOW_METHODS` | M√©todos HTTP permitidos | GET,POST,OPTIONS |
| `CORS_ALLOW_HEADERS` | Headers permitidos | Content-Type,Accept,User-Agent |
| `CORS_MAX_AGE` | Tiempo de cache CORS (segundos) | 3600 |

### Ejemplo de configuraci√≥n

```bash
# .env
API_HOST=0.0.0.0
API_PORT=8000
SCRAPING_INTERVAL_MINUTES=30
LOG_LEVEL=DEBUG
ENABLE_SCREENSHOTS=false
```

## üìù Logging

El sistema incluye logging detallado:

- **Archivo de logs**: `financial_api.log`
- **Niveles disponibles**: DEBUG, INFO, WARNING, ERROR
- **Informaci√≥n registrada**: 
  - Operaciones de scraping
  - Errores y excepciones
  - Requests a la API
  - Actualizaciones de datos

## üîß Troubleshooting

### Problemas comunes

1. **Error de Playwright**
```bash
playwright install chromium
```

2. **Error de permisos en Windows**
```bash
# Ejecutar como administrador o usar
playwright install --with-deps chromium
```

3. **Timeout en scraping**
- Aumentar `BROWSER_TIMEOUT` en configuraci√≥n
- Verificar conexi√≥n a internet
- Revisar logs para detalles espec√≠ficos

4. **Datos vac√≠os**
- Verificar que las URLs de scraping est√©n accesibles
- Revisar selectores CSS en los scrapers
- Habilitar screenshots para debugging

### Debugging

1. **Habilitar logs detallados**
```bash
LOG_LEVEL=DEBUG
```

2. **Habilitar screenshots**
```bash
ENABLE_SCREENSHOTS=true
```

3. **Verificar estado de la API**
```bash
curl http://localhost:8000/health
```

## üèóÔ∏è Arquitectura

```
financial_api/
‚îú‚îÄ‚îÄ main.py              # API principal con scheduler
‚îú‚îÄ‚îÄ api.py               # API para recibir datos
‚îú‚îÄ‚îÄ bot_scraper.py       # Bot de scraping independiente
‚îú‚îÄ‚îÄ data_store.py        # Almacenamiento de datos
‚îú‚îÄ‚îÄ config.py            # Configuraci√≥n centralizada
‚îú‚îÄ‚îÄ logger.py            # Sistema de logging
‚îú‚îÄ‚îÄ scraper/             # M√≥dulos de scraping
‚îÇ   ‚îú‚îÄ‚îÄ tradingview.py   # Scraper de TradingView
‚îÇ   ‚îú‚îÄ‚îÄ finviz.py        # Scraper de Finviz
‚îÇ   ‚îî‚îÄ‚îÄ yahoo.py         # Scraper de Yahoo Finance
‚îú‚îÄ‚îÄ screenshots/         # Screenshots para debugging
‚îú‚îÄ‚îÄ data.json           # Datos almacenados
‚îú‚îÄ‚îÄ data_backup.json    # Backup de datos
‚îî‚îÄ‚îÄ financial_api.log   # Archivo de logs
```

## ü§ù Contribuir

1. Fork el proyecto
2. Crear una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abrir un Pull Request

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para detalles.

## ‚ö†Ô∏è Disclaimer

Este proyecto es solo para fines educativos. Aseg√∫rate de cumplir con los t√©rminos de servicio de los sitios web que est√°s scrapeando. El uso de datos financieros debe cumplir con las regulaciones locales aplicables. 