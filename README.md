# Financial Data API

Una API robusta para obtener datos financieros de m√∫ltiples fuentes (TradingView, Finviz, Yahoo Finance) con scraping automatizado y persistencia de datos.

## üöÄ Caracter√≠sticas

- **M√∫ltiples fuentes de datos**: TradingView, Finviz, Yahoo Finance
- **Scraping automatizado**: Programado cada 50 minutos (configurable)
- **Persistencia de datos**: Almacenamiento en JSON con backup autom√°tico
- **Logging completo**: Sistema de logs detallado para debugging
- **API RESTful**: Endpoints para obtener datos y controlar scraping
- **Manejo robusto de errores**: Recuperaci√≥n autom√°tica y validaci√≥n de datos
- **Configuraci√≥n flexible**: Variables de entorno para personalizaci√≥n

## üìã Requisitos

- Python 3.8+
- Playwright (navegador automatizado)
- Conexi√≥n a internet

## üõ†Ô∏è Instalaci√≥n

1. **Clonar el repositorio**
```bash
git clone <tu-repositorio>
cd financial_api
```

2. **Crear entorno virtual**
```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate
```

3. **Instalar dependencias**
```bash
pip install -r requirements.txt
```

4. **Instalar navegadores para Playwright**
```bash
playwright install chromium
```

5. **Configurar variables de entorno (opcional)**
```bash
# Copiar archivo de ejemplo
cp .env.example .env
# Editar seg√∫n necesidades
```

## üöÄ Uso

### Opci√≥n 1: API Principal (Recomendado)

La API principal incluye scraping autom√°tico y scheduler:

```bash
# Iniciar la API
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

**Endpoints disponibles:**
- `GET /` - Informaci√≥n de la API
- `GET /datos` - Obtener todos los datos financieros
- `GET /datos/resume` - Obtener resumen de datos
- `POST /scrape` - Ejecutar scraping manualmente
- `GET /health` - Verificar estado de la API

### Opci√≥n 2: API Separada + Bot

Si prefieres separar el scraping de la API:

```bash
# Terminal 1: API para recibir datos
uvicorn api:app --host 0.0.0.0 --port 8000 --reload

# Terminal 2: Bot de scraping (ejecutar cuando necesites datos)
python bot_scraper.py
```

## üìä Estructura de Datos

Los datos se almacenan en la siguiente estructura:

```json
{
  "tradingview": {
    "indices": [...],
    "acciones": [...],
    "cripto": [...],
    "forex": [...]
  },
  "finviz": {
    "forex": [...],
    "acciones": [...],
    "indices": [...]
  },
  "yahoo": {
    "forex": [...],
    "acciones": [...],
    "materias_primas": [...],
    "indices": [...]
  },
  "last_updated": "2024-01-01T12:00:00",
  "metadata": {
    "version": "1.0",
    "created_at": "2024-01-01T00:00:00"
  }
}
```

## ‚öôÔ∏è Configuraci√≥n

### Variables de Entorno

| Variable | Descripci√≥n | Valor por defecto |
|----------|-------------|-------------------|
| `API_HOST` | Host de la API | localhost |
| `API_PORT` | Puerto de la API | 8000 |
| `SCRAPING_INTERVAL_MINUTES` | Intervalo de scraping | 50 |
| `REQUEST_TIMEOUT` | Timeout para requests | 15 |
| `BROWSER_TIMEOUT` | Timeout para navegador | 60000 |
| `DATA_FILE` | Archivo de datos | data.json |
| `BACKUP_FILE` | Archivo de backup | data_backup.json |
| `LOG_LEVEL` | Nivel de logging | INFO |
| `LOG_FILE` | Archivo de logs | financial_api.log |
| `ENABLE_SCREENSHOTS` | Habilitar screenshots | true |
| `CORS_ORIGINS` | Or√≠genes permitidos para CORS | http://localhost:3000,http://127.0.0.1:3000 |
| `CORS_ALLOW_CREDENTIALS` | Permitir credenciales en CORS | false |
| `CORS_ALLOW_METHODS` | M√©todos HTTP permitidos | GET,POST,OPTIONS |
| `CORS_ALLOW_HEADERS` | Headers permitidos | Content-Type,Accept,User-Agent |
| `CORS_MAX_AGE` | Tiempo de cache CORS (segundos) | 3600 |

### Ejemplo de configuraci√≥n

```bash
# .env
API_HOST=0.0.0.0
API_PORT=8000
SCRAPING_INTERVAL_MINUTES=30
LOG_LEVEL=DEBUG
ENABLE_SCREENSHOTS=false
```

## üìù Logging

El sistema incluye logging detallado:

- **Archivo de logs**: `financial_api.log`
- **Niveles disponibles**: DEBUG, INFO, WARNING, ERROR
- **Informaci√≥n registrada**: 
  - Operaciones de scraping
  - Errores y excepciones
  - Requests a la API
  - Actualizaciones de datos

## üîß Troubleshooting

### Problemas comunes

1. **Error de Playwright**
```bash
playwright install chromium
```

2. **Error de permisos en Windows**
```bash
# Ejecutar como administrador o usar
playwright install --with-deps chromium
```

3. **Timeout en scraping**
- Aumentar `BROWSER_TIMEOUT` en configuraci√≥n
- Verificar conexi√≥n a internet
- Revisar logs para detalles espec√≠ficos

4. **Datos vac√≠os**
- Verificar que las URLs de scraping est√©n accesibles
- Revisar selectores CSS en los scrapers
- Habilitar screenshots para debugging

### Debugging

1. **Habilitar logs detallados**
```bash
LOG_LEVEL=DEBUG
```

2. **Habilitar screenshots**
```bash
ENABLE_SCREENSHOTS=true
```

3. **Verificar estado de la API**
```bash
curl http://localhost:8000/health
```

## üèóÔ∏è Arquitectura

```
financial_api/
‚îú‚îÄ‚îÄ main.py              # API principal con scheduler
‚îú‚îÄ‚îÄ api.py               # API para recibir datos
‚îú‚îÄ‚îÄ bot_scraper.py       # Bot de scraping independiente
‚îú‚îÄ‚îÄ data_store.py        # Almacenamiento de datos
‚îú‚îÄ‚îÄ config.py            # Configuraci√≥n centralizada
‚îú‚îÄ‚îÄ logger.py            # Sistema de logging
‚îú‚îÄ‚îÄ scraper/             # M√≥dulos de scraping
‚îÇ   ‚îú‚îÄ‚îÄ tradingview.py   # Scraper de TradingView
‚îÇ   ‚îú‚îÄ‚îÄ finviz.py        # Scraper de Finviz
‚îÇ   ‚îî‚îÄ‚îÄ yahoo.py         # Scraper de Yahoo Finance
‚îú‚îÄ‚îÄ screenshots/         # Screenshots para debugging
‚îú‚îÄ‚îÄ data.json           # Datos almacenados
‚îú‚îÄ‚îÄ data_backup.json    # Backup de datos
‚îî‚îÄ‚îÄ financial_api.log   # Archivo de logs
```

## ü§ù Contribuir

1. Fork el proyecto
2. Crear una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abrir un Pull Request

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para detalles.

## ‚ö†Ô∏è Disclaimer

Este proyecto es solo para fines educativos. Aseg√∫rate de cumplir con los t√©rminos de servicio de los sitios web que est√°s scrapeando. El uso de datos financieros debe cumplir con las regulaciones locales aplicables. 