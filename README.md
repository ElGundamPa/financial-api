# Financial Data API

Una API robusta para obtener datos financieros de mÃºltiples fuentes (TradingView, Finviz, Yahoo Finance) con scraping automatizado y persistencia de datos.

## ğŸš€ CaracterÃ­sticas

- **MÃºltiples fuentes de datos**: TradingView, Finviz, Yahoo Finance
- **Scraping automatizado**: Programado cada 50 minutos (configurable)
- **Persistencia de datos**: Almacenamiento en JSON con backup automÃ¡tico
- **Logging completo**: Sistema de logs detallado para debugging
- **API RESTful**: Endpoints para obtener datos y controlar scraping
- **Manejo robusto de errores**: RecuperaciÃ³n automÃ¡tica y validaciÃ³n de datos
- **ConfiguraciÃ³n flexible**: Variables de entorno para personalizaciÃ³n

## ğŸ“‹ Requisitos

- Python 3.8+
- Playwright (navegador automatizado)
- ConexiÃ³n a internet

## ğŸ› ï¸ InstalaciÃ³n

1. **Clonar el repositorio**
```bash
git clone <tu-repositorio>
cd financial_api
```

2. **Crear entorno virtual**
```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate
```

3. **Instalar dependencias**
```bash
pip install -r requirements.txt
```

4. **Instalar navegadores para Playwright**
```bash
playwright install chromium
```

5. **Configurar variables de entorno (opcional)**
```bash
# Copiar archivo de ejemplo
cp .env.example .env
# Editar segÃºn necesidades
```

## ğŸš€ Uso

### OpciÃ³n 1: API Principal (Recomendado)

La API principal incluye scraping automÃ¡tico y scheduler:

```bash
# Iniciar la API
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

**Endpoints disponibles:**
- `GET /` - InformaciÃ³n de la API
- `GET /datos` - Obtener todos los datos financieros
- `GET /datos/summary` - Obtener resumen de datos
- `POST /scrape` - Ejecutar scraping manualmente
- `GET /health` - Verificar estado de la API

### OpciÃ³n 2: API Separada + Bot

Si prefieres separar el scraping de la API:

```bash
# Terminal 1: API para recibir datos
uvicorn api:app --host 0.0.0.0 --port 8000 --reload

# Terminal 2: Bot de scraping (ejecutar cuando necesites datos)
python bot_scraper.py
```

## ğŸ“Š Estructura de Datos

Los datos se almacenan en la siguiente estructura:

```json
{
  "tradingview": {
    "indices": [...],
    "acciones": [...],
    "cripto": [...],
    "forex": [...]
  },
  "finviz": {
    "forex": [...],
    "acciones": [...],
    "indices": [...]
  },
  "yahoo": {
    "forex": [...],
    "acciones": [...],
    "materias_primas": [...],
    "indices": [...]
  },
  "last_updated": "2024-01-01T12:00:00",
  "metadata": {
    "version": "1.0",
    "created_at": "2024-01-01T00:00:00"
  }
}
```

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno

| Variable | DescripciÃ³n | Valor por defecto |
|----------|-------------|-------------------|
| `API_HOST` | Host de la API | localhost |
| `API_PORT` | Puerto de la API | 8000 |
| `SCRAPING_INTERVAL_MINUTES` | Intervalo de scraping | 50 |
| `REQUEST_TIMEOUT` | Timeout para requests | 15 |
| `BROWSER_TIMEOUT` | Timeout para navegador | 60000 |
| `DATA_FILE` | Archivo de datos | data.json |
| `BACKUP_FILE` | Archivo de backup | data_backup.json |
| `LOG_LEVEL` | Nivel de logging | INFO |
| `LOG_FILE` | Archivo de logs | financial_api.log |
| `ENABLE_SCREENSHOTS` | Habilitar screenshots | true |

### Ejemplo de configuraciÃ³n

```bash
# .env
API_HOST=0.0.0.0
API_PORT=8000
SCRAPING_INTERVAL_MINUTES=30
LOG_LEVEL=DEBUG
ENABLE_SCREENSHOTS=false
```

## ğŸ“ Logging

El sistema incluye logging detallado:

- **Archivo de logs**: `financial_api.log`
- **Niveles disponibles**: DEBUG, INFO, WARNING, ERROR
- **InformaciÃ³n registrada**: 
  - Operaciones de scraping
  - Errores y excepciones
  - Requests a la API
  - Actualizaciones de datos

## ğŸ”§ Troubleshooting

### Problemas comunes

1. **Error de Playwright**
```bash
playwright install chromium
```

2. **Error de permisos en Windows**
```bash
# Ejecutar como administrador o usar
playwright install --with-deps chromium
```

3. **Timeout en scraping**
- Aumentar `BROWSER_TIMEOUT` en configuraciÃ³n
- Verificar conexiÃ³n a internet
- Revisar logs para detalles especÃ­ficos

4. **Datos vacÃ­os**
- Verificar que las URLs de scraping estÃ©n accesibles
- Revisar selectores CSS en los scrapers
- Habilitar screenshots para debugging

### Debugging

1. **Habilitar logs detallados**
```bash
LOG_LEVEL=DEBUG
```

2. **Habilitar screenshots**
```bash
ENABLE_SCREENSHOTS=true
```

3. **Verificar estado de la API**
```bash
curl http://localhost:8000/health
```

## ğŸ—ï¸ Arquitectura

```
financial_api/
â”œâ”€â”€ main.py              # API principal con scheduler
â”œâ”€â”€ api.py               # API para recibir datos
â”œâ”€â”€ bot_scraper.py       # Bot de scraping independiente
â”œâ”€â”€ data_store.py        # Almacenamiento de datos
â”œâ”€â”€ config.py            # ConfiguraciÃ³n centralizada
â”œâ”€â”€ logger.py            # Sistema de logging
â”œâ”€â”€ scraper/             # MÃ³dulos de scraping
â”‚   â”œâ”€â”€ tradingview.py   # Scraper de TradingView
â”‚   â”œâ”€â”€ finviz.py        # Scraper de Finviz
â”‚   â””â”€â”€ yahoo.py         # Scraper de Yahoo Finance
â”œâ”€â”€ screenshots/         # Screenshots para debugging
â”œâ”€â”€ data.json           # Datos almacenados
â”œâ”€â”€ data_backup.json    # Backup de datos
â””â”€â”€ financial_api.log   # Archivo de logs
```

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crear una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abrir un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para detalles.

## âš ï¸ Disclaimer

Este proyecto es solo para fines educativos. AsegÃºrate de cumplir con los tÃ©rminos de servicio de los sitios web que estÃ¡s scrapeando. El uso de datos financieros debe cumplir con las regulaciones locales aplicables. 